<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Aditya Parameshwaran</title>
  <meta name="author" content="Aditya Parameshwaran">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.3.0/font/bootstrap-icons.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
  
  <!-- Your custom stylesheet -->
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <div class="container-fluid page-container">
    <div class="row">
      <!-- Sidebar Navigation -->
      <nav class="col-md-3 col-lg-2 d-md-block sidebar">
        <div class="position-sticky sticky-top pt-5">
          <ul class="nav flex-column">
            <li class="nav-item">
              <a class="nav-link active" href="#">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#research">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#projects">Projects</a>
            </li>
          </ul>
        </div>
      </nav>

      <!-- Main Content -->
      <main class="col-md-9 col-lg-10 ms-sm-auto px-md-4">
        <div class="container pt-5 pb-5">
          <!-- Profile Section -->
          <section class="content-container">
            <div class="row align-items-center">
              <div class="col-lg-8">
                <h1 class="mb-4">Aditya Parameshwaran</h1>
                <div class="mb-4">
                  <p>
                    I am a PhD. candidate in the <a href="https://www.clemson.edu/cecas/departments/me/index.html"
                      target="_blank">Mechanical Engineering</a> department at Clemson University. I work at the <a
                      href="https://yue6.people.clemson.edu/index.php" target="_blank">Interdisciplinary Intelligent
                      Research Laboratory</a> at Clemson University under <a
                      href="https://www.clemson.edu/cecas/departments/me/people/faculty/wang.html" target="_blank">Dr. Yue
                      "Sophie" Wang</a>. I have also worked with a larger consortium of researchers at <a
                      href="https://www.clemson.edu/cecas/vipr-gs/index.html" target="_blank">VIPR-GS</a> group with the
                    Automotive Department at <a href="https://cuicar.com/" target="_blank">CU-ICAR</a>. Before this, I
                    completed my M.S. from Mechanical Engineering at Purdue University and my B.E. in
                    Mechanical Engineering at University of Pune.
                  </p>
                </div>
                <div class="contact-links text-center mb-4">
                  <a href="mailto:adiparamesh@gmail.com" target="_blank"><i class="bi bi-envelope-fill fs-4"></i></a>
                  <a href="./data/AdityaResume.pdf" target="_blank">CV</a>
                  <a href="https://scholar.google.com/citations?user=Oz75U7wAAAAJ&hl=en&oi=ao" target="_blank">Scholar</a>
                  <a href="https://github.com/aparame" target="_blank"><i class="bi bi-github fs-4"></i></a>
                </div>
              </div>
              <div class="col-lg-4">
                <img alt="profile photo" src="./images/AdityaProfile.png" class="profile-image">
              </div>
            </div>
          </section>

          <!-- Education Section -->
          <section class="content-container">
            <h3 class="section-heading">Education</h3>
            <div class="list-group list-group-flush">
              <div class="list-group-item">
                <div class="d-flex justify-content-between align-items-center">
                  <div>
                    <h5 class="education-heading mb-1">BE Mechanical Engineering</h5>
                    <p class="mb-0">MIT - Pune</p>
                  </div>
                  <span class="badge bg-light text-dark">2015 - 2018</span>
                </div>
              </div>
              <div class="list-group-item">
                <div class="d-flex justify-content-between align-items-center">
                  <div>
                    <h5 class="education-heading mb-1">MS Mechanical Engineering</h5>
                    <p class="mb-0"><i>Purdue University</i></p>
                  </div>
                  <span class="badge bg-light text-dark">2019 - 2021</span>
                </div>
              </div>
              <div class="list-group-item">
                <div class="d-flex justify-content-between align-items-center">
                  <div>
                    <h5 class="education-heading mb-1">PhD Candidate Mechanical Engineering</h5>
                    <p class="mb-0">Clemson University</p>
                  </div>
                  <span class="badge bg-light text-dark">2022 - Present</span>
                </div>
              </div>
            </div>
          </section>

          <!-- Research Section -->
          <section class="content-container" id="research">
            <h3 class="section-heading">Research</h3>
            <p style="text-align: justify;" class="mb-4">
              My research interests lie in neurosymbolic deep learning, formal verification and controls for robotics and
              autonomous vehicle applications.
              I have worked on developing path planning and navigation tools for ground robots using formal tools like
              temporal languages <a href="https://github.com/aparame/Temporal_Logic_Guided_Robot_Navigation">[1]</a> <a
                href="https://github.com/aparame/Faster_TLG-RN">[2]</a>.
              I have also worked on integrating 3D semantic mapping tools for off-road ground robot applications using the
              <a href="https://octomap.github.io/" target="_blank">octomap</a> library.
              Currently, I am developing neurosymbolic tools to formally verify convolutional neural networks and neural
              network controllers for complex dynamical systems.
            </p>

            <div class="research-items">
              <!-- Research Item 1 -->
              <div class="card research-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="dual-media-container">
                      <div class="media-item">
                        <img src="images/research/images/safety-verification.png" alt="Safety Verification" class="research-image">
                      </div>
                      <div class="media-item">
                        <img src="images/research/mw_2.gif" alt="Safety Verification Animation" class="animated-gif">
                      </div>
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Oz75U7wAAAAJ&citation_for_view=Oz75U7wAAAAJ:u5HHmVD_uO8C"
                          target="_blank" class="text-decoration-none">
                          <span class="papertitle">Safety Verification of Autonomous Vehicles based on Signal Temporal Logic (STL) constraints</span>
                        </a>
                      </h5>
                      <p class="card-text">
                        <a href="https://aparame.github.io/aditya_website/" class="text-decoration-none"><b>Aditya Parameshwaran</b></a>,
                        <a href="https://yue6.people.clemson.edu/index.php" class="text-decoration-none">Yue Wang</a>
                      </p>
                      <p class="card-text">
                        2D navigation model for an autonomous vehicle based on task specifications given in signal
                        temporal logic (STL) guaranteeing safety. This work is presented in the SAE WCX 2023 conference at Detroit.
                      </p>
                      <div class="mt-3">
                        <a href="https://drive.google.com/file/d/1-9hnOEj0WTeBWehGWztxvabiHVWpZPeI/view" target="_blank"
                          class="btn btn-sm btn-outline-primary me-2">Paper</a>
                        <a href="https://docs.google.com/presentation/d/1iYR8xlaLWW_LBy45rsd_VEJTerPy_VyL/edit?usp=sharing&ouid=115261715409316286458&rtpof=true&sd=true"
                          target="_blank" class="btn btn-sm btn-outline-primary me-2">Slides</a>
                        <a href="https://github.com/aparame/Temporal_Logic_Guided_Robot_Navigation" target="_blank"
                          class="btn btn-sm btn-outline-primary">Source Code</a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Research Item 2 -->
              <div class="card research-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="image-container">
                      <img src="images/research/images/complex_3goal.jpg" alt="Temporal Logic" class="research-image">
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <span class="papertitle">Temporal Logic Guided Robot Navigation</span>
                      </h5>
                      <p class="card-text">
                        <b>Aditya Parameshwaran</b>, Yue Wang
                      </p>
                      <p class="card-text">
                        2D controller synthesis combining Linear and Signal Temporal Logic specifications to gurantee safe
                        and robust navigation for ground robots. This method is an update on the SAE Paper from 2023 and
                        is faster while maintaining similar levels of safety as before. It is published as part of the IFAC papers in the MECC 2024 conference.
                      </p>
                      <div class="mt-3">
                        <a href="#" target="_blank" class="btn btn-sm btn-outline-primary me-2">Paper</a>
                        <a href="https://github.com/aparame/Faster_TLG-RN" target="_blank" class="btn btn-sm btn-outline-primary">Source Code</a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Research Item 3 -->
              <div class="card research-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="image-container">
                      <img src="images/research/images/approach.png" alt="Terrain Analysis" class="research-image">
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <span class="papertitle">Real-Time Terrain Analysis and Control for off-road Mobile Robots</span>
                      </h5>
                      <p class="card-text">
                        Edwina Lewis, <b>Aditya Parameshwaran</b>
                      </p>
                      <p class="card-text">
                        Bayesian Calibration Routine based off-road terrain roughness estimator combined with Simplex controller for mobile robots. This work is applied on
                        NVIDIA's Isaac Sim environment along with Jackal robot to collect IMU data and predict the
                        roughness of the terrain. The roughness estimates allow a Simplex controller to switch between performance and safety modes of operation. This work is part of the
                        SAE WCX 2025 Conference at Detroit.
                      </p>
                      <div class="mt-3">
                        <a href="#" target="_blank" class="btn btn-sm btn-outline-primary me-2">Paper</a>
                        <a href="#" target="_blank" class="btn btn-sm btn-outline-primary">Source Code</a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>

          <!-- Projects Section -->
          <section class="content-container" id="projects">
            <h3 class="section-heading">Projects</h3>
            <p class="mb-4">
              I have been involved in various robotics projects since completing my MS at Purdue, some in collaboration with companies like <a href="#project-railbot">Wabtec Corporation</a>,
              and others as side projects for the <a href="#project-octomap">US Army VIPR Centre</a>. These projects have spanned areas such as <a href="#project-controls">Controls</a>,
              <a href="#project-dl">Deep Learning</a>, <a href="#project-AV">Autonomous Navigation</a>, and <a href="#project-cv">Computer Vision</a>.
            </p>

            <div class="project-items">
              <!-- Project Item 1 -->
              <div id="project-railbot" class="card project-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="image-container">
                      <img src="images/research/images/railbot.jpg" alt="Railway Robot" class="project-image">
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <span class="papertitle">Autonomous Train Robot for Track Health Monitoring</span>
                      </h5>
                      <div class="mb-2">
                        <span class="tech-tag">ROS</span>
                        <span class="tech-tag">Python</span>
                        <span class="tech-tag">PyTorch</span>
                        <span class="tech-tag">Embedded C</span>
                      </div>
                      <p class="card-text">
                        At Purdue University, I contributed to the development of an autonomous railway bot that navigated tracks autonomously while collecting sensor
                        data using LiDAR, stereo cameras, and IMU. The bot created 3D maps of its surroundings and could carry a 30-pound payload for additional sensors.
                        An NVIDIA Jetson AGX processed the data, which was used to train a CNN for traffic sign recognition.
                      </p>
                      <a href="https://github.com/aparame/Autonomous_RailBot" target="_blank" class="btn btn-sm btn-outline-primary">Source Code</a>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Project Item 2 -->
              <div id="project-AV" class="card project-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="image-container">
                      <img src="images/research/images/rc_car.jpg" alt="RC Car" class="project-image">
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <span class="papertitle">Autonomous RC Car with Traffic Sign Identification using CNN's</span>
                      </h5>
                      <div class="mb-2">
                        <span class="tech-tag">MATLAB</span>
                        <span class="tech-tag">PyTorch</span>
                        <span class="tech-tag">Embedded C</span>
                        <span class="tech-tag">OpenCV</span>
                      </div>
                      <p class="card-text">
                        As part of a group project, we developed an autonomous lane-keeping and sign-detecting RC car. The track layout was identified, and optimal camera
                        placements were chosen. Image data was processed using OpenCV for lane-keeping. A ResNet-based CNN was trained to detect
                        traffic signs, and the RC car successfully navigated the track, responding accurately to signs.
                      </p>
                      <div class="mt-3">
                        <a href="https://drive.google.com/file/d/1PvhqNhtHjqh15I8arVBqJ329xoe5nkOL/view" target="_blank" class="btn btn-sm btn-outline-primary me-2">Paper</a>
                        <a href="https://github.com/aparame/AI_RC_Car" target="_blank" class="btn btn-sm btn-outline-primary me-2">Source Code</a>
                        <a href="https://docs.google.com/presentation/d/1CcWqkM2MQmbkFz47-91WTZGl0ZzYxxNljZTX2ZMmXg8/edit#slide=id.p1" target="_blank" class="btn btn-sm btn-outline-primary">Slides</a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Project Item 3 -->
              <div id="project-octomap" class="card project-card">
                <div class="row g-0">
                  <div class="col-md-4">
                    <div class="image-container">
                      <img src="images/research/octomap.gif" alt="Octomap GIF" class="animated-gif">
                    </div>
                  </div>
                  <div class="col-md-8">
                    <div class="card-body">
                      <h5 class="card-title">
                        <span class="papertitle">Semantic Segmentation and 3D Map Generation for Outdoor Environments</span>
                      </h5>
                      <div class="mb-2">
                        <span class="tech-tag">C++</span>
                        <span class="tech-tag">Python</span>
                        <span class="tech-tag">ROS2</span>
                        <span class="tech-tag">Sensor Fusion</span>
                      </div>
                      <p class="card-text">
                        Integrated stereo cameras and LiDAR on a Husky robot using ROS2 in Unity simulation environment to develop semantically segmented 3D maps of the environment.
                        Used the octomap library to fuse semantically segmented RGB data from the camera with LiDAR point cloud to generate 3D maps using voxels.
                      </p>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>

          <!-- Footer / Map -->
          <div class="footer-map">
            <a href="https://clustrmaps.com/site/1bzsm" title="Visit tracker">
              <img src="//www.clustrmaps.com/map_v2.png?d=CohQDJ03hYE1HZTkcj3jQZrHggxMQALIKVQO070CtSM&cl=ffffff" alt="Visitor Map" />
            </a>
          </div>
        </div>
      </main>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
    crossorigin="anonymous"></script>

  <!-- Lightbox Modal -->
  <div id="imageModal" class="modal">
    <span class="close">&times;</span>
    <div class="modal-content">
      <img id="modalImage">
    </div>
    <div id="caption"></div>
  </div>

  <script>
    // Get the modal
    const modal = document.getElementById("imageModal");
    const modalImg = document.getElementById("modalImage");
    const captionText = document.getElementById("caption");
    const closeBtn = document.getElementsByClassName("close")[0];
    
    // Function to initialize clickable images
    function initLightbox() {
      // Get all images that should be clickable
      const images = document.querySelectorAll('.research-image, .project-image, .animated-gif, .profile-image');
      
      // Add click event to each image
      images.forEach(img => {
        img.addEventListener('click', function() {
          modal.style.display = "block";
          modalImg.src = this.src;
          captionText.innerHTML = this.alt;
          
          // Disable scrolling on the body when modal is open
          document.body.style.overflow = "hidden";
        });
      });
      
      // Close the modal when clicking the Ã— button
      closeBtn.addEventListener('click', closeModal);
      
      // Close the modal when clicking outside the image
      modal.addEventListener('click', function(event) {
        if (event.target === modal) {
          closeModal();
        }
      });
      
      // Close modal with Escape key
      document.addEventListener('keydown', function(event) {
        if (event.key === "Escape") {
          closeModal();
        }
      });
    }
    
    function closeModal() {
      modal.style.display = "none";
      // Re-enable scrolling
      document.body.style.overflow = "auto";
    }
    
    // Initialize the lightbox after the page loads
    document.addEventListener('DOMContentLoaded', initLightbox);
    </script>
</body>

</html>